{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransUnet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMz3Cp1XsJ+73VKPpDYu1f+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os"],"metadata":{"id":"Zcr8osDGWOy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xA6UKaw8TbWT"},"outputs":[],"source":["input_dire = \"Rim/image/\"\n","target_dire = \"Rim/label/\"\n","\n","input_img_RIM = sorted(\n","    [\n","        os.path.join(input_dire, fname)\n","        for fname in os.listdir(input_dire)\n","        if fname.endswith(\".jpg\")\n","        \n","    ]\n",")\n","target_img_RIM = sorted(\n","    [\n","        os.path.join(target_dire, fname)\n","        for fname in os.listdir(target_dire)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")"]},{"cell_type":"code","source":["cont = 1\n","from PIL import Image"],"metadata":{"id":"7U17OKxuWVcs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x in range(len(input_img_RIM)):\n","  original = Image.open(input_img_RIM[x])\n","  original_Mask = Image.open(target_img_RIM[x])\n","  width, height = original.size\n","  if (width != 2144):\n","    wid = 1500\n","    h = 1424\n","    x = 520\n","  else:\n","    wid = width/2\n","    h = height\n","    x = 0\n","  imghalf = original.crop((x, 0, wid, h))\n","  imghalf_Mask = original_Mask.crop((x, 0, wid, h))  \n","  imghalf.save('data/image/'+str(cont)+'.png')\n","  imghalf_Mask.save('data/label/'+str(cont)+'.png')\n","  cont = cont+1"],"metadata":{"id":"LzkOWOHcWV0z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# https://github.com/mkara44/transunet_pytorch/blob/main/utils/vit.py"],"metadata":{"id":"53VjZ6IVXSg1"}},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-t6QnLdXnSV","executionInfo":{"status":"ok","timestamp":1642680197853,"user_tz":180,"elapsed":3480,"user":{"displayName":"Pedro Victor Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhUpa2gTxsTDh_UBzgBvRP6NnBUPMMj5SPoT2DEA=s64","userId":"18290251612393282293"}},"outputId":"39faba49-fdec-4306-e327-feaef52aaeee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from einops import rearrange, repeat\n"],"metadata":{"id":"zFxAIy6DXOfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, embedding_dim, head_num):\n","        super().__init__()\n","\n","        self.head_num = head_num\n","        self.dk = (embedding_dim // head_num) ** 1 / 2\n","\n","        self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n","        self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n","\n","    def forward(self, x, mask=None):\n","        qkv = self.qkv_layer(x)\n","\n","        query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n","        energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n","\n","        if mask is not None:\n","            energy = energy.masked_fill(mask, -np.inf)\n","\n","        attention = torch.softmax(energy, dim=-1)\n","\n","        x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n","\n","        x = rearrange(x, \"b h t d -> b t (h d)\")\n","        x = self.out_attention(x)\n","\n","        return x\n","\n"],"metadata":{"id":"-YUy7zqhXuZ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, embedding_dim, mlp_dim):\n","        super().__init__()\n","\n","        self.mlp_layers = nn.Sequential(\n","            nn.Linear(embedding_dim, mlp_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(mlp_dim, embedding_dim),\n","            nn.Dropout(0.1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.mlp_layers(x)\n","\n","        return x\n"],"metadata":{"id":"3BvpMZwCXzmV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderBlock(nn.Module):\n","    def __init__(self, embedding_dim, head_num, mlp_dim):\n","        super().__init__()\n","\n","        self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n","        self.mlp = MLP(embedding_dim, mlp_dim)\n","\n","        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n","        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        _x = self.multi_head_attention(x)\n","        _x = self.dropout(_x)\n","        x = x + _x\n","        x = self.layer_norm1(x)\n","\n","        _x = self.mlp(x)\n","        x = x + _x\n","        x = self.layer_norm2(x)\n","\n","        return x"],"metadata":{"id":"LPtS_P6TX2zW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n","        super().__init__()\n","\n","        self.layer_blocks = nn.ModuleList(\n","            [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n","\n","    def forward(self, x):\n","        for layer_block in self.layer_blocks:\n","            x = layer_block(x)\n","\n","        return x"],"metadata":{"id":"8yN6QziQX54N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ViT(nn.Module):\n","    def __init__(self, img_dim, in_channels, embedding_dim, head_num, mlp_dim,\n","                 block_num, patch_dim, classification=True, num_classes=1):\n","        super().__init__()\n","\n","        self.patch_dim = patch_dim\n","        self.classification = classification\n","        self.num_tokens = (img_dim // patch_dim) ** 2\n","        self.token_dim = in_channels * (patch_dim ** 2)\n","\n","        self.projection = nn.Linear(self.token_dim, embedding_dim)\n","        self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n","\n","        if self.classification:\n","            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        img_patches = rearrange(x,\n","                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n","                                patch_x=self.patch_dim, patch_y=self.patch_dim)\n","\n","        batch_size, tokens, _ = img_patches.shape\n","\n","        project = self.projection(img_patches)\n","        token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n","                       batch_size=batch_size)\n","\n","        patches = torch.cat([token, project], dim=1)\n","        patches += self.embedding[:tokens + 1, :]\n","\n","        x = self.dropout(patches)\n","        x = self.transformer(x)\n","        x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n","\n","        return x"],"metadata":{"id":"ALZbPgxAX9FO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderBottleneck(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n","        super().__init__()\n","\n","        self.downsample = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","        width = int(out_channels * (base_width / 64))\n","\n","        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n","        self.norm1 = nn.BatchNorm2d(width)\n","\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n","        self.norm2 = nn.BatchNorm2d(width)\n","\n","        self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n","        self.norm3 = nn.BatchNorm2d(out_channels)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x_down = self.downsample(x)\n","\n","        x = self.conv1(x)\n","        x = self.norm1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.norm2(x)\n","        x = self.relu(x)\n","\n","        x = self.conv3(x)\n","        x = self.norm3(x)\n","        x = x + x_down\n","        x = self.relu(x)\n","\n","        return x\n"],"metadata":{"id":"ZiPqHP_ZX_l-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderBottleneck(nn.Module):\n","    def __init__(self, in_channels, out_channels, scale_factor=2):\n","        super().__init__()\n","\n","        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x, x_concat=None):\n","        x = self.upsample(x)\n","\n","        if x_concat is not None:\n","            x = torch.cat([x_concat, x], dim=1)\n","\n","        x = self.layer(x)\n","        return x"],"metadata":{"id":"BrHXhwe_YJWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.norm1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n","        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n","        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n","\n","        self.vit_img_dim = img_dim // patch_dim\n","        self.vit = ViT(self.vit_img_dim, out_channels * 8, out_channels * 8,\n","                       head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n","\n","        self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n","        self.norm2 = nn.BatchNorm2d(512)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.norm1(x)\n","        x1 = self.relu(x)\n","\n","        x2 = self.encoder1(x1)\n","        x3 = self.encoder2(x2)\n","        x = self.encoder3(x3)\n","\n","        x = self.vit(x)\n","        x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_img_dim, y=self.vit_img_dim)\n","\n","        x = self.conv2(x)\n","        x = self.norm2(x)\n","        x = self.relu(x)\n","\n","        return x, x1, x2, x3\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, out_channels, class_num):\n","        super().__init__()\n","\n","        self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n","        self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n","        self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n","        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n","\n","        self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n","\n","    def forward(self, x, x1, x2, x3):\n","        x = self.decoder1(x, x3)\n","        x = self.decoder2(x, x2)\n","        x = self.decoder3(x, x1)\n","        x = self.decoder4(x)\n","        x = self.conv1(x)\n","\n","        return x\n","\n","\n","class TransUNet(nn.Module):\n","    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n","        super().__init__()\n","\n","        self.encoder = Encoder(img_dim, in_channels, out_channels,\n","                               head_num, mlp_dim, block_num, patch_dim)\n","\n","        self.decoder = Decoder(out_channels, class_num)\n","\n","    def forward(self, x):\n","        x, x1, x2, x3 = self.encoder(x)\n","        x = self.decoder(x, x1, x2, x3)\n","\n","        return x\n"],"metadata":{"id":"CwBRVHdnYM1O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransUNetSeg:\n","    def __init__(self):\n","        self.model = TransUNet(img_dim=256,\n","                               in_channels=3,\n","                               out_channels=256,\n","                               head_num=4,\n","                               mlp_dim=512,\n","                               block_num=4,\n","                               patch_dim=8,\n","                               class_num=1)\n","\n","        # self.criterion = dice_loss\n","        # self.optimizer = SGD(self.model.parameters(), lr=cfg.learning_rate,\n","        #                      momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n","     \n","\n"],"metadata":{"id":"s63wiGOTYQfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TransUNetSeg()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"VJNv0VANaD15","executionInfo":{"status":"error","timestamp":1642681002778,"user_tz":180,"elapsed":1365,"user":{"displayName":"Pedro Victor Fonseca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhUpa2gTxsTDh_UBzgBvRP6NnBUPMMj5SPoT2DEA=s64","userId":"18290251612393282293"}},"outputId":"2c2865d0-11da-4699-8c3c-1dcefd45e0d5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-f4cd0817360d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransUNetSeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'TransUNetSeg' object has no attribute 'summary'"]}]}]}